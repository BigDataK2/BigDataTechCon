# Needed Camus properties, more cleanup to come
#
# Almost all properties have decent default properties. When in doubt, comment out the property.
#

# The job name.
camus.job.name=BigDataTechCon Tutorial

# final top-level data output directory, sub-directory will be dynamically created for each topic pulled
# etl.destination.path=/user/username/topics
etl.destination.path=/tmp/tutorial
etl.destination.path.topic.sub.dirformat='filtered_data'

# HDFS location where you want to keep execution files, i.e. offsets, error logs, and count files
etl.execution.base.path=/tmp/camus/filtered_data
# where completed Camus job output directories are kept, usually a sub-dir in the base.path
etl.execution.history.path=/tmp/camus/filtered_data/history

# Decoder class can also be set on a per topic basis.
camus.message.decoder.class=com.conversantmedia.tutorial.camus.CsvMessageDecoder

etl.record.writer.provider.class=com.conversantmedia.tutorial.camus.CsvRecordWriterProvider

mapred.job.tracker=local
fs.default.name=file:///
mapreduce.job.local.dir=/tmp/mkeane/jobdir
mapreduce.framework.name=local

# Used by avro-based Decoders (KafkaAvroMessageDecoder and LatestSchemaKafkaAvroMessageDecoder) to use as their schema registry.
# Out of the box options are:
# com.linkedin.camus.schemaregistry.FileSchemaRegistry
# com.linkedin.camus.schemaregistry.MemorySchemaRegistry
# com.linkedin.camus.schemaregistry.AvroRestSchemaRegistry
# com.linkedin.camus.example.schemaregistry.DummySchemaRegistry
kafka.message.coder.schema.registry.class=com.linkedin.camus.example.schemaregistry.StaticSchemaRegistry

# Used by the committer to arrange .avro files into a partitioned scheme. This will be the default partitioner for all
# topic that do not have a partitioner specified.
# Out of the box options are (for all options see the source for configuration options):
# com.linkedin.camus.etl.kafka.partitioner.HourlyPartitioner, groups files in hourly directories
# com.linkedin.camus.etl.kafka.partitioner.DailyPartitioner, groups files in daily directories
# com.linkedin.camus.etl.kafka.partitioner.TimeBasedPartitioner, groups files in very configurable directories
# com.linkedin.camus.etl.kafka.partitioner.DefaultPartitioner, like HourlyPartitioner but less configurable
# com.linkedin.camus.etl.kafka.partitioner.TopicGroupingPartitioner
etl.partitioner.class=com.linkedin.camus.etl.kafka.partitioner.DailyPartitioner


# all files in this dir will be added to the distributed cache and placed on the classpath for hadoop tasks
# hdfs.default.classpath.dir=

# max hadoop tasks to use, each task can pull multiple topic partitions
mapred.map.tasks=20
# max historical time that will be pulled from each partition based on event timestamp
kafka.max.pull.hrs=-1
# events with a timestamp older than this will be discarded.
kafka.max.historical.days=-1
# Max minutes for each mapper to pull messages (-1 means no limit)
kafka.max.pull.minutes.per.task=-1
# Just go to earliest offset?
#kafka.move.to.earliest.offset=true
# Just go to last offset?
#kafka.move.to.last.offset.list=filtered_data

# if whitelist has values, only whitelisted topic are pulled. Nothing on the blacklist is pulled
kafka.blacklist.topics=
kafka.whitelist.topics=filtered_data
log4j.configuration=true

# Name of the client as seen by kafka
kafka.client.name=camus
# The Kafka brokers to connect to, format: kafka.brokers=host1:port,host2:port,host3:port
# Production Brokers
kafka.brokers=localhost:9092


#Stops the mapper from getting inundated with Decoder exceptions for the same topic
#Default value is set to 10
max.decoder.exceptions.to.print=5

#Controls the submitting of counts to Kafka
#Default value set to true
post.tracking.counts.to.kafka=false
monitoring.event.class=class.that.generates.record.to.submit.counts.to.kafka

# everything below this point can be ignored for the time being, will provide more documentation down the road
##########################
etl.run.tracking.post=false
kafka.monitor.tier=
etl.counts.path=
kafka.monitor.time.granularity=10

etl.hourly=hourly
etl.daily=daily

# Should we ignore events that cannot be decoded (exception thrown by MessageDecoder)?
# `false` will fail the job, `true` will silently drop the event.
etl.ignore.schema.errors=false

# configure output compression for deflate or snappy. Defaults to deflate
#mapred.output.compress=true
mapred.output.compress=false
etl.output.codec=snappy
#etl.output.codec=deflate
etl.deflate.level=6



etl.output.file.time.partition.mins=60
etl.keep.count.files=false
etl.execution.history.max.of.quota=.8

mapred.map.max.attempts=1

kafka.client.buffer.size=20971520
kafka.client.so.timeout=60000

#zookeeper.session.timeout=
#zookeeper.connection.timeout=

alert.email.host=localhost
alert.email.port=25
#alert.email.addresses=
alert.email.subject=BigDataTechCon Camus Job
alert.email.sender=Camus@conversantmedia.com

etl.max.percent.skipped.other=0
etl.max.percent.skipped.schemanotfound=0
