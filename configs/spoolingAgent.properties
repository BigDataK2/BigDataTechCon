# Define components of the spooling agent
spoolingAgent.sources = s1
spoolingAgent.channels = kafka-channel-1
spoolingAgent.sinks = kafka-sink-1

# A spooldir source will poll a directory for files to flume
spoolingAgent.sources.s1.type=spooldir
# Where you drop files for flume spoolingDir agent to ingest
spoolingAgent.sources.s1.spoolDir=/tmp/BigDataTechCon/spool
# What "channel" to persist the read in data too
spoolingAgent.sources.s1.channels=kafka-channel-1

# Use KafkaChannel for persisting to assure delivery
# Data pesisted here is stored as a protobuf
spoolingAgent.channels.kafka-channel-1.type = org.apache.flume.channel.kafka.KafkaChannel
spoolingAgent.channels.kafka-channel-1.brokerList = localhost:9092
spoolingAgent.channels.kafka-channel-1.topic = spooling_agent_channel
spoolingAgent.channels.kafka-channel-1.zookeeperConnect = localhost:2181


# Using a KafkaSink, this wil be the raw text read in
spoolingAgent.sinks.kafka-sink-1.type = org.apache.flume.sink.kafka.KafkaSink
# What channel should the sink read from?
spoolingAgent.sinks.kafka-sink-1.channel = kafka-channel-1
# Transaction size
spoolingAgent.sinks.kafka-sink-1.batchSize = 100
# For failover purposes normally list at least 3 brokers, for demo purposes just 1
spoolingAgent.sinks.kafka-sink-1.brokerList = localhost:9092
# What Topic are we sinking to
spoolingAgent.sinks.kafka-sink-1.topic = data_in
